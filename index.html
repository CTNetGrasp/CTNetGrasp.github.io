<!DOCTYPE html>
<html>
    <head>
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, viewport-fit=cover" name="viewport" />
    <link rel="alternate icon" type="image/png" href="https://xiejialong.github.io/CTNet/img/favicon.png">
    <title>paper | </title>
    
<link rel="stylesheet" href="https://xiejialong.github.io/CTNet/css/reset.css">

    
<link rel="stylesheet" href="https://xiejialong.github.io/CTNet/css/style.css">

    
<link rel="stylesheet" href="https://xiejialong.github.io/CTNet/css/markdown.css">

    
<link rel="stylesheet" href="https://xiejialong.github.io/CTNet/css/fonts.css">


    <link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.css">
<meta name="generator" content="Hexo 6.3.0"></head>
    <body>
        <div class="paper">
            <div class="paper-main">

                <div class="post-list">
    
    <div class="post">
        <!-- <a class="post-title" href="https://xiejialong.github.io/CTNet/2023/08/21/paper/">Listen, Perceive, Grasp : CLIP-driven attribute-aware network for visual-language segmentation and grasping detection</a> -->
        <div class="post-title" >Listen, Perceive, Grasp : CLIP-driven attribute-aware network for visual-language segmentation and grasping detection</div>
        <div class="post-authors" >Jialong Xie, Jialong Xie, Jialong Xie, Jialong Xie,</div>
        <div class="head-icon"> 
            
            
                <a class="icon-container" target="_blank" rel="noopener" href="https://www.github.com">
                    <i class="fa fa-github-square"></i>
                </a>
                 
            
            
                <a class="icon-container" target="_blank" rel="noopener" href="https://www.youtube.com">
                    <i class="fa fa-youtube-square"></i>
                </a>
                 
            
        </div>

        <div class="post-abstract" >Visual-language grasping is a fundamental and challenging task in robotics manipulation and human-robot collaboration. However, previous approaches not only overlook fine-grained visual perception but also neglect the correlation between object attributes and candidate grasping detection. To this end, we propose a CLIP-driven attribute-aware network (CTNet) for robotic visual-language segmentation and grasping detection, enabling the robots to listen, perceive, and grasp in real-world applications. The listen stage leverages the pre-trained CLIP to understand and capture linguistic concepts. The perceive stage serves as mining object-orientated features and attributes (e.g. boundary and spatial location) as well as yielding the fine-grained segmentation mask. This stage can provide a prior constraint for grasping detection. The grasp stage aggregates the perceived attribute information to constrain and refine the spatial location and width of the grasping rectangle, facilitating the generation of a high-quality grasp pose. Lastly, we provide an extended large dataset refOCIDGrasp to train and test our method, resulting in a grasping accuracy of 97.76% and segmentation OIoU of 81.82%. The real-world robotic applications demonstrate the effectiveness and generalization of our proposed approach.</div>
        
        
            <div class="video-container">
                <iframe
                width="560"
                height="315"
                src="https://www.youtube.com/embed/aVLP5_yC8l0"
                frameborder="0"
                allowfullscreen
                ></iframe>
            </div>
        
        <div class="post-except">
            <h3 id="The-listen-perceive-grasp-paradigm-for-robotic-grasp-reasoning"><a href="#The-listen-perceive-grasp-paradigm-for-robotic-grasp-reasoning" class="headerlink" title="The listen-perceive-grasp paradigm for robotic grasp reasoning"></a>The <em>listen-perceive-grasp</em> paradigm for robotic grasp reasoning</h3><p><img src="https://xiejialong.github.io/CTNet/images/Schematic.jpg" alt="listen-perceive-grasp paradigm"><br><br /></p>
<h3 id="Overview-of-CTNet"><a href="#Overview-of-CTNet" class="headerlink" title="Overview of CTNet"></a>Overview of CTNet</h3><p><img src="https://xiejialong.github.io/CTNet/images/architecture.jpg" alt="listen-perceive-grasp paradigm"><br><img src="https://xiejialong.github.io/CTNet/images/TAMMI.jpg" alt="TAMMI"></p>
<br />

<h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><p><img src="https://xiejialong.github.io/CTNet/images/visualization.jpg" alt="Visualization"></p>
<!-- ### Real-world robotic grasping experiments
<iframe src="//www.youtube.com/embed/IfjVsa1t2Jw" frameborder="0" allowfullscreen="" width='640' height='480'></iframe> -->
            <!-- <a class="read-more" href="https://xiejialong.github.io/CTNet/2023/08/21/paper/"> ... </a> -->
        </div>
        <div class="post-date">2023.08.21</div>
    </div>
    
</div>

                <div class="footer">
    <span>Copyright © 2022 paper</span>
    <span>Theme Designed By <a target="_blank" href="https://zheli.design/one-paper">這李設計</a></span>
</div>


<link rel="stylesheet" href="https://xiejialong.github.io/CTNet/css/a11y-dark.min.css">


<script src="https://xiejialong.github.io/CTNet/js/highlight.min.js"></script>


<script src="https://xiejialong.github.io/CTNet/js/highlightjs-line-numbers.js"></script>


<script>
    hljs.initHighlightingOnLoad();
    hljs.initLineNumbersOnLoad();
</script>

            </div>
        </div>
    </body>
</html>